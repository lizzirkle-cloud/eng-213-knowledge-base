---
title:Dangers of Biases in GenAI
layout: page 
---
# Dangers of Biases in GenAI
Generative AI systems create content based on what they "learn." This can have a negative impact as "the training data has been shown to have problematic characteristics resulting in models that encode stereotypical and derogatory associations along gender, race, ethnicity, and disability status" (O'Neil).
Throughout our readings and class discussions it is apparent that AI being filtered and flagged by humans can cause biases to be implemented into the programs. When people decide what should be shown and produced by AI it raises the question of who should get to decide what AI generates. Additionally, this has a lot of dangers and negative implications for those that choose to use AI, especially if they fall into the category of those being silenced. 
### 1. Promotion of Harmful and Limited Ideals

   In programming GenAI resources some harmful ideals can be promoted, while inclusivity of other perspectives and culturals are removed from content. 

   An example of this can be found when feeding GenAi writing prompts, "ChatGPT advances linguistic homogeneity and white language supremacy by designating Ghanaian English as “non-standard” and by wrongly claiming that it is inappropriate for an academic paper" (Sano-Franchini). This removal of any language that does not fall into the way GenAI was programmed is automatically removed, showing how harmful ideals and only allows for a narrow scope of ideas and language to be shown. 
   
### 2. Separation of Marginalized Groups 
While researching facial recognitions, researcher Buolamwini, found that dark-skinned females were likely to be misclassified. 
This issue has many negative implications. First, people that have this physical characteristic are unable to be detected through their phones and on AI models/ recognitions. Second, "biometrics — the use of faces and other physical characteristics for identification — are increasingly being used for education, health care, and policing" (O'Neil). When people are being mis-identified there could be wrongful accusations and faulty judgements that only target marginalized groups. 
### 3. Removal of Content/ Information
AI content must be filtered and moderated, which is an issue in many different ways. This is a problem as many poor, marginzlied communities and being under-paid to watch disturbing content in order to moderate AI. This is harmful to the people that are having to interact with explicit conent and can lead to mental health issues.
Additionally, moderators may be told to flag or remove certain content that may be promoting the works or ideals of communities that are being silenced. This will lead to content removal and the promotion of "white supremacy" narratives and harmful AI agendas.
